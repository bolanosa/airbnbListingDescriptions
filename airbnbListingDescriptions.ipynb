{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airbnb is an online community marketplace for people to book a complete travel experience, including accommodations and locally curated activities. One of the first interactions a guest has with a home is the listing description. The listing refers to the description of the home, and provides an opportunity to highlight unique features. As it turns out, the listing description occupies a decisive role in how guests perceive the overall quality of a home. It is so important that homes qualifying for the Airbnb Plus program (a collection of homes verified for quality, comfort and style) have their listing descriptions written by professional writers trained in the Airbnb Plus editorial style. But writing a unique and captivating listing description, that stands out from other homes can be difficult. Airbnb offers resources for writing a great listing description on their [blog](https://blog.atairbnb.com/writing-great-listing-descriptions/), and also includes [tips](https://www.airbnb.com/help/article/431/how-do-i-make-my-listing-more-competitive) on how to make a listing competitive. \n",
    "\n",
    "However, this can be difficult to implement in practice for many hosts, who have no way of receiving feedback until after the guest leaves a rating. To help hosts maximize the probability of earning a high rating, I propose a text classification system that will allow hosts to receive instant feedback on their listing descriptions. The classifier is built on top of Google's state-of-the-art pre-trained model, BERT, a bidirectional transformer model. BERT is powerful because it understands the contextual information to the left and right of a word, giving it a deeper understanding of the language model. BERTs outputs were fed into a deep feedforward neural network with a dropout layer (to enforce regularization) and a softmax classifier. Trained on roughly 7,000 listing descriptions in San Francisco, the model achieved 81.3% accuracy in classifying the star rating a host will receive based on their listing description. In addition, the data showed that guests value private homes above corporate properties, and prefer listing descriptions that emphasize the unique qualities of a home rather than a list of generic amenities. Accordingly, hosts are recommended to focus on these unique characteristics in their descriptions, and include words that evoke homey feelings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is available at [Inside Airbnb](http://insideairbnb.com/san-francisco/#), a site that collects detailed information from some of the major cities Airbnb operates in. For this project, the data was downloaded from the San Francisco dataset as a series of csv files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Scrub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data for exploratory analysis, which primarily entails handling missing values by discretization and replacing null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>price</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>review_scores_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New update: the house next door is under const...</td>\n",
       "      <td>t</td>\n",
       "      <td>37.76931</td>\n",
       "      <td>-122.43386</td>\n",
       "      <td>170</td>\n",
       "      <td>180</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We live in a large Victorian house on a quiet ...</td>\n",
       "      <td>f</td>\n",
       "      <td>37.74511</td>\n",
       "      <td>-122.42102</td>\n",
       "      <td>235</td>\n",
       "      <td>111</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nice and good public transportation.  7 minute...</td>\n",
       "      <td>f</td>\n",
       "      <td>37.76669</td>\n",
       "      <td>-122.45250</td>\n",
       "      <td>65</td>\n",
       "      <td>17</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice and good public transportation.  7 minute...</td>\n",
       "      <td>f</td>\n",
       "      <td>37.76487</td>\n",
       "      <td>-122.45183</td>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pls email before booking.  Interior featured i...</td>\n",
       "      <td>f</td>\n",
       "      <td>37.77525</td>\n",
       "      <td>-122.43637</td>\n",
       "      <td>785</td>\n",
       "      <td>27</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description host_is_superhost  \\\n",
       "0  New update: the house next door is under const...                 t   \n",
       "1  We live in a large Victorian house on a quiet ...                 f   \n",
       "2  Nice and good public transportation.  7 minute...                 f   \n",
       "3  Nice and good public transportation.  7 minute...                 f   \n",
       "4  Pls email before booking.  Interior featured i...                 f   \n",
       "\n",
       "   latitude  longitude  price  number_of_reviews  review_scores_rating  \n",
       "0  37.76931 -122.43386    170                180                  97.0  \n",
       "1  37.74511 -122.42102    235                111                  98.0  \n",
       "2  37.76669 -122.45250     65                 17                  85.0  \n",
       "3  37.76487 -122.45183     65                  8                  93.0  \n",
       "4  37.77525 -122.43637    785                 27                  97.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "listing = pd.read_csv('listing.csv')\n",
    "listing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert review_scores_rating to a 5 point scale\n",
    "listing['review_scores_rating'] = listing['review_scores_rating']/20\n",
    "\n",
    "# discretize review_scores_rating\n",
    "listing['review_scores_rating'] = pd.cut(listing['review_scores_rating'], bins=5,\n",
    "                                         labels=['0.0-1.0', '1.0-2.0', '2.0-3.0', '3.0-4.0', '4.0-5.0'])\n",
    "\n",
    "# replace null values in description and review_scores_rating\n",
    "listing['description'].replace(np.NaN, 'no_description', inplace=True)\n",
    "listing['review_scores_rating'].replace(np.NaN, 'no_review', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **review_scores_rating** is scored on a 100-point scale. To keep the data in line with what hosts generally see, convert this value to a 5-point scale, the star rating that hosts are accustomed to. The **review_scores_rating** is equivalent to the **overall star rating**, an aggregated score for each category a guest can give a rating on. \n",
    "\n",
    "\n",
    "* Discretize **review_scores_rating** to handle any missing data by replacing with **no_review**. We can't impute these missing values since they are intentionally missing because the guest did not leave a review.\n",
    "\n",
    "\n",
    "* Likewise, empty fields in **description** have been deliberately left blank by the host and can not be imputed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Geospatial analysis of star ratings and price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[kepler.gl](http://kepler.gl/) is a geospatial analysis tool developed by Uber's Data Vis team. It functions best with large-scale datasets, but also works beautifully with smaller ones. Simply drag and drop a dataset into the web-based app, and then begin applying filters, layers and colors to bring geospatial data to life. \n",
    "\n",
    "Set the color in the hexbin price layer by the **median** price instead of the **average** to avoid misrepresenting the data. Since the average is more sensitive to extreme values, we want to avoid artificially inflating the price of an area. Polar price points in a given area, are not uncommon in San Francisco where wealthy neighborhoods are often bordered by less affluent ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![keplerPlot](imgs/kepler-gl.png)\n",
    "**Fig. 1** A layered hexbin and point map of ratings and price of Airbnb listings in San Francisco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Characteristic terms and associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, there aren't very many good options for text visualization. Word clouds are perhaps the most common method, but they can be hard to interpret. It can be difficult to compare the sizes of two non-horizontally adjacent words, and longer words can appear to have an outsized impact simply because they occupy more space. \n",
    "\n",
    "To avoid these pitfalls, [Scattertext](https://github.com/JasonKessler/scattertext) is a visualization tool that presents a scatterplot, where each axis corresponds to the frequency of a term and its related category. The term associations are determined by their scaled F-score, the harmonic mean of precision and frequency. The scaled F-score takes the normal cumulative density function of the precision and frequency scores, to scale and standardize both scores. Terms that are associated will have both high category precision and frequency, producing a higher scaled F-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot guide\n",
    "* Terms used commonly in all listing descriptions are located in the upper-right corner of the plot (home, house, kitchen, apartment, room). These words are all common elements of a living space regardless of high or low star rating. On the other hand, terms used infrequently in all listing descriptions are located in the lower-left corner of the plot. For example, children is rarely used, which suggests homes accommodating children are relatively rare. \n",
    "\n",
    "\n",
    "* It gets more interesting when we look at the upper-left and bottom-right corners. These are terms that are most commonly associated with high star ratings and all other ratings respectively. The colors also help identify word association. Those terms that are more aligned with high star ratings are blue, and those more associated with all other ratings are red.  \n",
    "\n",
    "\n",
    "* Scattertext is an [interactive plot](https://textexplorer.herokuapp.com/), and clicking on a word brings up an excerpt from where it was used in the listing description. There is also a search function if you want to look up a specific word. Finally, Scattertext is unable to compare multiple groups at once, which is why we need to take a one-to-many visualization approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6852412"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries for handling text\n",
    "import scattertext as st\n",
    "import spacy\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from pprint import pprint\n",
    "\n",
    "# load English model\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# create a scattertext corpus & look for differences between listing descriptions in ratings\n",
    "# remove stop words\n",
    "corpus = (st.CorpusFromPandas(listing,\n",
    "                              category_col='review_scores_rating',\n",
    "                              text_col='description',\n",
    "                              nlp=nlp)\n",
    "          .build()\n",
    "          .remove_terms(ENGLISH_STOP_WORDS, ignore_absences=True))\n",
    "\n",
    "# generate scatterplot and save to HTML file\n",
    "# set a minimum term frequency to filter out infrequent words\n",
    "html = st.produce_scattertext_explorer(corpus,\n",
    "          category='4.0-5.0',\n",
    "          category_name='Star Rating 4.0 - 5.0',\n",
    "          not_category_name='All Other Ratings',\n",
    "          width_in_pixels=1000,\n",
    "          minimum_term_frequency=200)\n",
    "open('term-associations.html', 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![termAssociations](imgs/termAssociations.png)\n",
    "**Fig. 2** Scatterplot of terms associated with high star ratings and all other ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Topic signals and categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is helpful to understand what types of topics are being mentioned in listing descriptions that correspond with high star ratings. Hosts can model and incorporate these topic categories into their own listing descriptions, while learning which ones to avoid.\n",
    "\n",
    "[Empath](https://hci.stanford.edu/publications/2016/ethan/empath-chi-2016.pdf) allows us to learn the topic signals within listing descriptions, by generating and validating categories from a few seed terms. For example, the terms vintage, trendy and fashion would fall under the hipster category in Empath. Empath works by learning the word representations from 1.8 billion words from fiction stories with a neural network, and creates a vector space that measures the cosine similarity between words. Given a set of new seed terms, Empath utilizes its learned neural weights to discover related terms, and then generates a new lexical category.\n",
    "\n",
    "Integrating Empath with Scattertext, we can build a scatterplot of topics associated with high star ratings and all other ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10617716"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize Empath topics/categories\n",
    "feat_builder = st.FeatsFromOnlyEmpath()\n",
    "\n",
    "# create corpus from Empath\n",
    "empath_corpus = (st.CorpusFromParsedDocuments(listing,\n",
    "                                             category_col='review_scores_rating',\n",
    "                                             feats_from_spacy_doc=feat_builder,\n",
    "                                             parsed_col='description')\n",
    "                .build())\n",
    "\n",
    "# generate scatterplot and save to HTML file\n",
    "# set use_non_text_features=True to use labeled Empath topics/categories\n",
    "# set topic_model_term_lists=feat_builder.get_top_model_term_lists to ensure relevant terms are bolded in excerpts\n",
    "html = st.produce_scattertext_explorer(empath_corpus,\n",
    "                                       category='4.0-5.0',\n",
    "                                       category_name='Star Rating 4.0-5.0',\n",
    "                                       not_category_name='All Other Ratings',\n",
    "                                       width_in_pixels=1000,\n",
    "                                       use_non_text_features=True,\n",
    "                                       use_full_doc=True,\n",
    "                                       topic_model_term_lists=feat_builder.get_top_model_term_lists())\n",
    "open('term-Empath.html', 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![termEmpath](imgs/term-Empath.png)\n",
    "**Fig. 3** A scatterplot of lexical categories associated with high star ratings and all other ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Word similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may also be interesting to create a plot of word similarities to visualize how listing descriptions with high star ratings differ in words used. A word2vec model was used to generate word embeddings of the listing descriptions, and then tSNE (t-distributed stochastic neighbor embedding) was used to reduce dimensionality.\n",
    "\n",
    "A word2vec model is simply a vector representation of a word, and maps words to a vector of real numbers (otherwise known as a word embedding). Two common word embedding models are the continuous bag of words model and the skip-gram model. The latter assumes that context words are generated from a central target word, while the former assumes the opposite.\n",
    "\n",
    "Generating word embeddings can create highly dimensional data, which is why its useful to employ a dimensionality reduction technique like tSNE. tSNE is an implementation of manifold learning, a technique for learning non-linear structure in data. The advantages of tSNE include reducing tendency of crowding points around the center, and exposing natural clusters within the data. Read [here](https://lvdmaaten.github.io/tsne/) for more details about tSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8111972"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# split sentences in listing description with st.whitespace_nlp_with_sentences\n",
    "listing['parse'] = listing['description'].apply(st.whitespace_nlp_with_sentences)\n",
    "\n",
    "# create a stop-listed corpus of unigram terms\n",
    "corpus = (st.CorpusFromParsedDocuments(listing, \n",
    "                                       category_col='review_scores_rating', \n",
    "                                       parsed_col='parse')\n",
    "          .build()\n",
    "          .get_stoplisted_unigram_corpus())\n",
    "\n",
    "# generate scatterplot and save to HTML file\n",
    "# learn word embeddings with a word2vec model (specify more worker threads for faster processing)\n",
    "# use t-SNE projection model to visualize word similarity in two-dimensions\n",
    "html = st.produce_projection_explorer(corpus,\n",
    "                                      word2vec_model=Word2Vec(workers=4),\n",
    "                                      projection_model=TSNE(),\n",
    "                                      category='4.0-5.0',\n",
    "                                      category_name='Star Rating 4.0-5.0',\n",
    "                                      not_category_name='All Other Ratings')  \n",
    "open('wordSimilarity.html', 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tSNE](imgs/tSNE.png)\n",
    "**Fig. 4** Word similarity clusters are revealed with tSNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional Encoder Representations from Transformers ([BERT](https://github.com/google-research/bert)) is a state-of-the-art model released by Google in late 2018. It is a method for adapting pre-trained language representations for downstream NLP tasks. What makes BERT truly unique is its bidirectionality, meaning, it captures contextual information to the left and right of a word. Training bidirectional models are inherently difficult though because it would allow the word that's being predicted to indirectly see itself in a multi-layered context. This hurdle is overcome by randomly masking some words, and then asking BERT to predict the masked word. To train BERT to understand relationships between two sentences, during the pre-training process, BERT is further tasked with predicting whether one sentence follows another.\n",
    "\n",
    "There are two stages in using BERT: pre-training and fine-tuning.\n",
    "\n",
    "* **Pre-training** is the process of training the BERT model on an extremely large text corpus (Wikipedia) to learn language representations. This is a highly expensive task (four days of training on 4-16 cloud TPUs!), but thankfully, the researchers at Google have already completed this step and have publicly released their pre-trained models. All we have to do is download these pre-trained models. For this project, we will use the Uncased BERT-Base model, which simply means all the text has been lowercased, and accent markers have been stripped. The BERT-Base model architecture contains 12 Transformer blocks, 768 hidden units, and 12 attention heads. \n",
    "\n",
    "* **Fine-tuning** is the stage where we adapt the BERT model to our classification task. On a CPU, this process took 14 hrs, so we recommend using Google's Colaboratory notebook, and accelerate the process by taking advantage of their cloud GPUs.\n",
    "\n",
    "The following code is adapted from Google's [BERT FineTuning with Cloud TPU](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb) notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common in NLP to tokenize text such that each word is a separate entity. BERT uses [WordPiece model](https://arxiv.org/pdf/1609.08144.pdf) tokenization, where each word is further segmented into sub-word units. A special **[CLS]** (classifier) token is appended to the beginning of the text, and a **[SEP]** (separator) token is inserted at the end. The WordPiece tokens are generated by optimizing the number of tokens, such that the resulting corpus contains the minimal number of WordPieces when segmented according to the model. By creating sub-word units, WordPiece allows BERT to capture out-of-vocabulary words and store only 30,522 words, which effectively gives it an evolving word dictionary. \n",
    "\n",
    "In summary, WordPiece tokenization achieves a good balance between vocabulary size and rare words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in /anaconda3/lib/python3.6/site-packages (1.0.1)\r\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from bert-tensorflow) (1.11.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0325 21:19:27.208451 4584494528 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "# install BERT\n",
    "!pip install bert-tensorflow\n",
    "\n",
    "# import necessary libraries for BERT\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "\n",
    "# set output directory to store model output and checkpoints\n",
    "output_dir = 'modelOutput'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# encode categorical variables\n",
    "listing['review_scores_rating'] = LabelEncoder().fit_transform(listing['review_scores_rating'])\n",
    "\n",
    "# create training and test sets\n",
    "train, test = train_test_split(listing, test_size=0.25)\n",
    "\n",
    "# create feature and target variables\n",
    "# create a list of labels\n",
    "DATA_COLUMN = 'description'\n",
    "LABEL_COLUMN = 'review_scores_rating'\n",
    "label_list = [0, 1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data into format BERT understands\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n",
    "                                                                             text_a=x[DATA_COLUMN],\n",
    "                                                                             text_b=None,\n",
    "                                                                             label=x[LABEL_COLUMN]), axis = 1)\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n",
    "                                                                           text_a=x[DATA_COLUMN],\n",
    "                                                                           text_b=None,\n",
    "                                                                           label=x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:56.035194 4584494528 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# create a tokenizer from uncased BERT model\n",
    "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1'\n",
    "\n",
    "def create_tokenizer_from_hub():\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module=hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info=bert_module(signature='tokenization_info', as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info['vocab_file'],\n",
    "                                                  tokenization_info['do_lower_case']])\n",
    "    return bert.tokenization.FullTokenizer(\n",
    "    vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.225611 4584494528 tf_logging.py:115] Writing example 0 of 5363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.232403 4584494528 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.234306 4584494528 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] private garden du ##plex w / beautiful cozy bedroom upstairs and lounge down plus elegant en suite bathroom . separate entrance and access to huge landscape ##d garden and patio . safe , quiet neighborhood near g ##g park , museums , transit lines and retail . private , quiet , cozy 2 story space with comfortable single bed ( sleeps one ) . has separate entrance and access to large walled garden . neighborhood has pl ##ent ##iful non - permit parking . no kitchen access . rental has mini fridge , electric kettle , instant coffee / tea and snacks provided . limited interaction due to coming / going and separation of main house from guest cottage . close to golden gate park , museums , uc ##sf , shops , restaurants , retail fun and everything wonderful on the west side of sf . we are close to many bus lines and the n judah streetcar . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.239041 4584494528 tf_logging.py:115] tokens: [CLS] private garden du ##plex w / beautiful cozy bedroom upstairs and lounge down plus elegant en suite bathroom . separate entrance and access to huge landscape ##d garden and patio . safe , quiet neighborhood near g ##g park , museums , transit lines and retail . private , quiet , cozy 2 story space with comfortable single bed ( sleeps one ) . has separate entrance and access to large walled garden . neighborhood has pl ##ent ##iful non - permit parking . no kitchen access . rental has mini fridge , electric kettle , instant coffee / tea and snacks provided . limited interaction due to coming / going and separation of main house from guest cottage . close to golden gate park , museums , uc ##sf , shops , restaurants , retail fun and everything wonderful on the west side of sf . we are close to many bus lines and the n judah streetcar . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2797 3871 4241 19386 1059 1013 3376 26931 5010 8721 1998 11549 2091 4606 11552 4372 7621 5723 1012 3584 4211 1998 3229 2000 4121 5957 2094 3871 1998 19404 1012 3647 1010 4251 5101 2379 1043 2290 2380 1010 9941 1010 6671 3210 1998 7027 1012 2797 1010 4251 1010 26931 1016 2466 2686 2007 6625 2309 2793 1006 25126 2028 1007 1012 2038 3584 4211 1998 3229 2000 2312 17692 3871 1012 5101 2038 20228 4765 18424 2512 1011 9146 5581 1012 2053 3829 3229 1012 12635 2038 7163 16716 1010 3751 22421 1010 7107 4157 1013 5572 1998 27962 3024 1012 3132 8290 2349 2000 2746 1013 2183 1998 8745 1997 2364 2160 2013 4113 9151 1012 2485 2000 3585 4796 2380 1010 9941 1010 15384 22747 1010 7340 1010 7884 1010 7027 4569 1998 2673 6919 2006 1996 2225 2217 1997 16420 1012 2057 2024 2485 2000 2116 3902 3210 1998 1996 1050 23022 21420 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.240581 4584494528 tf_logging.py:115] input_ids: 101 2797 3871 4241 19386 1059 1013 3376 26931 5010 8721 1998 11549 2091 4606 11552 4372 7621 5723 1012 3584 4211 1998 3229 2000 4121 5957 2094 3871 1998 19404 1012 3647 1010 4251 5101 2379 1043 2290 2380 1010 9941 1010 6671 3210 1998 7027 1012 2797 1010 4251 1010 26931 1016 2466 2686 2007 6625 2309 2793 1006 25126 2028 1007 1012 2038 3584 4211 1998 3229 2000 2312 17692 3871 1012 5101 2038 20228 4765 18424 2512 1011 9146 5581 1012 2053 3829 3229 1012 12635 2038 7163 16716 1010 3751 22421 1010 7107 4157 1013 5572 1998 27962 3024 1012 3132 8290 2349 2000 2746 1013 2183 1998 8745 1997 2364 2160 2013 4113 9151 1012 2485 2000 3585 4796 2380 1010 9941 1010 15384 22747 1010 7340 1010 7884 1010 7027 4569 1998 2673 6919 2006 1996 2225 2217 1997 16420 1012 2057 2024 2485 2000 2116 3902 3210 1998 1996 1050 23022 21420 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.242115 4584494528 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.243913 4584494528 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.245398 4584494528 tf_logging.py:115] label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.253696 4584494528 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.261648 4584494528 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] newly remodeled modern in - law unit right by bus and mu ##ni line . 2 blocks away from golden gate park and less than a mile away from the pacific ocean , this one bedroom in san francisco ’ s sunset district provides you and your family or friends plenty of space and tran ##quil ##ity after a busy day of exploring . it is the 3rd house down from n - judah , which runs between 6 ##am - 12 ##am : you can hear it - light sleeper ##s may not like it . after going through the garage with your own private access , you ’ ll enter the living room , a din ##ning table and a fully e ##qui ##ped kitchen . in the living room , you have a very comfortable sleeper ( full - xl ) , to enjoy our 55 \" 4 ##k tv and apple tv . please note , we do not have cable , but joining the wi - fi will allow you to easily stream anything from an apple computer or mobile device . the bedroom has a queen bed and a walk - in closet to store your clothes . it overlook ##s the fence ##d yard . we also encourage you to enjoy the backyard of the unit , finished with chairs and a table . g [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.266741 4584494528 tf_logging.py:115] tokens: [CLS] newly remodeled modern in - law unit right by bus and mu ##ni line . 2 blocks away from golden gate park and less than a mile away from the pacific ocean , this one bedroom in san francisco ’ s sunset district provides you and your family or friends plenty of space and tran ##quil ##ity after a busy day of exploring . it is the 3rd house down from n - judah , which runs between 6 ##am - 12 ##am : you can hear it - light sleeper ##s may not like it . after going through the garage with your own private access , you ’ ll enter the living room , a din ##ning table and a fully e ##qui ##ped kitchen . in the living room , you have a very comfortable sleeper ( full - xl ) , to enjoy our 55 \" 4 ##k tv and apple tv . please note , we do not have cable , but joining the wi - fi will allow you to easily stream anything from an apple computer or mobile device . the bedroom has a queen bed and a walk - in closet to store your clothes . it overlook ##s the fence ##d yard . we also encourage you to enjoy the backyard of the unit , finished with chairs and a table . g [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 4397 27170 2715 1999 1011 2375 3131 2157 2011 3902 1998 14163 3490 2240 1012 1016 5991 2185 2013 3585 4796 2380 1998 2625 2084 1037 3542 2185 2013 1996 3534 4153 1010 2023 2028 5010 1999 2624 3799 1521 1055 10434 2212 3640 2017 1998 2115 2155 2030 2814 7564 1997 2686 1998 25283 26147 3012 2044 1037 5697 2154 1997 11131 1012 2009 2003 1996 3822 2160 2091 2013 1050 1011 23022 1010 2029 3216 2090 1020 3286 1011 2260 3286 1024 2017 2064 2963 2009 1011 2422 24372 2015 2089 2025 2066 2009 1012 2044 2183 2083 1996 7381 2007 2115 2219 2797 3229 1010 2017 1521 2222 4607 1996 2542 2282 1010 1037 11586 5582 2795 1998 1037 3929 1041 15549 5669 3829 1012 1999 1996 2542 2282 1010 2017 2031 1037 2200 6625 24372 1006 2440 1011 28712 1007 1010 2000 5959 2256 4583 1000 1018 2243 2694 1998 6207 2694 1012 3531 3602 1010 2057 2079 2025 2031 5830 1010 2021 5241 1996 15536 1011 10882 2097 3499 2017 2000 4089 5460 2505 2013 2019 6207 3274 2030 4684 5080 1012 1996 5010 2038 1037 3035 2793 1998 1037 3328 1011 1999 9346 2000 3573 2115 4253 1012 2009 27590 2015 1996 8638 2094 4220 1012 2057 2036 8627 2017 2000 5959 1996 16125 1997 1996 3131 1010 2736 2007 8397 1998 1037 2795 1012 1043 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.268367 4584494528 tf_logging.py:115] input_ids: 101 4397 27170 2715 1999 1011 2375 3131 2157 2011 3902 1998 14163 3490 2240 1012 1016 5991 2185 2013 3585 4796 2380 1998 2625 2084 1037 3542 2185 2013 1996 3534 4153 1010 2023 2028 5010 1999 2624 3799 1521 1055 10434 2212 3640 2017 1998 2115 2155 2030 2814 7564 1997 2686 1998 25283 26147 3012 2044 1037 5697 2154 1997 11131 1012 2009 2003 1996 3822 2160 2091 2013 1050 1011 23022 1010 2029 3216 2090 1020 3286 1011 2260 3286 1024 2017 2064 2963 2009 1011 2422 24372 2015 2089 2025 2066 2009 1012 2044 2183 2083 1996 7381 2007 2115 2219 2797 3229 1010 2017 1521 2222 4607 1996 2542 2282 1010 1037 11586 5582 2795 1998 1037 3929 1041 15549 5669 3829 1012 1999 1996 2542 2282 1010 2017 2031 1037 2200 6625 24372 1006 2440 1011 28712 1007 1010 2000 5959 2256 4583 1000 1018 2243 2694 1998 6207 2694 1012 3531 3602 1010 2057 2079 2025 2031 5830 1010 2021 5241 1996 15536 1011 10882 2097 3499 2017 2000 4089 5460 2505 2013 2019 6207 3274 2030 4684 5080 1012 1996 5010 2038 1037 3035 2793 1998 1037 3328 1011 1999 9346 2000 3573 2115 4253 1012 2009 27590 2015 1996 8638 2094 4220 1012 2057 2036 8627 2017 2000 5959 1996 16125 1997 1996 3131 1010 2736 2007 8397 1998 1037 2795 1012 1043 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.270314 4584494528 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.272045 4584494528 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.273830 4584494528 tf_logging.py:115] label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.283876 4584494528 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.285717 4584494528 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] sophisticated condo appointed for corporate and temporary stays . convenient ##ly located on the corner of sans ##ome and pine streets . the royal exchange insurance building is an historic san francisco landmark . this unique building was the first commercial building completed downtown after the 1906 earthquake . in 2006 , it was converted to condom ##inium ##s . the brick and stone facade remain unchanged . the marble and glass entry is also an exquisite time piece . much of the marble was imported from milan , italy . marble staircase ##s are still in use between floors . there is a gorgeous common roof terrace with mature flowering and edible plants , seating and lou ##nging areas . this tenth floor unit is in the rear of the building , and has excellent sunlight . the original warehouse style bay windows are an historic part of this unit . a dream setting . san francisco living at it ' s best . this unit is approximately 560 square feet and is a perfect retreat for work or visit to san francisco . the ho [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.287419 4584494528 tf_logging.py:115] tokens: [CLS] sophisticated condo appointed for corporate and temporary stays . convenient ##ly located on the corner of sans ##ome and pine streets . the royal exchange insurance building is an historic san francisco landmark . this unique building was the first commercial building completed downtown after the 1906 earthquake . in 2006 , it was converted to condom ##inium ##s . the brick and stone facade remain unchanged . the marble and glass entry is also an exquisite time piece . much of the marble was imported from milan , italy . marble staircase ##s are still in use between floors . there is a gorgeous common roof terrace with mature flowering and edible plants , seating and lou ##nging areas . this tenth floor unit is in the rear of the building , and has excellent sunlight . the original warehouse style bay windows are an historic part of this unit . a dream setting . san francisco living at it ' s best . this unit is approximately 560 square feet and is a perfect retreat for work or visit to san francisco . the ho [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 12138 25805 2805 2005 5971 1998 5741 12237 1012 14057 2135 2284 2006 1996 3420 1997 20344 8462 1998 7222 4534 1012 1996 2548 3863 5427 2311 2003 2019 3181 2624 3799 8637 1012 2023 4310 2311 2001 1996 2034 3293 2311 2949 5116 2044 1996 5518 8372 1012 1999 2294 1010 2009 2001 4991 2000 20910 27585 2015 1012 1996 5318 1998 2962 8508 3961 15704 1012 1996 7720 1998 3221 4443 2003 2036 2019 19401 2051 3538 1012 2172 1997 1996 7720 2001 10964 2013 6954 1010 3304 1012 7720 10714 2015 2024 2145 1999 2224 2090 8158 1012 2045 2003 1037 9882 2691 4412 11134 2007 9677 10902 1998 21006 4264 1010 10747 1998 10223 22373 2752 1012 2023 7891 2723 3131 2003 1999 1996 4373 1997 1996 2311 1010 1998 2038 6581 9325 1012 1996 2434 9746 2806 3016 3645 2024 2019 3181 2112 1997 2023 3131 1012 1037 3959 4292 1012 2624 3799 2542 2012 2009 1005 1055 2190 1012 2023 3131 2003 3155 21267 2675 2519 1998 2003 1037 3819 7822 2005 2147 2030 3942 2000 2624 3799 1012 1996 7570 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.289232 4584494528 tf_logging.py:115] input_ids: 101 12138 25805 2805 2005 5971 1998 5741 12237 1012 14057 2135 2284 2006 1996 3420 1997 20344 8462 1998 7222 4534 1012 1996 2548 3863 5427 2311 2003 2019 3181 2624 3799 8637 1012 2023 4310 2311 2001 1996 2034 3293 2311 2949 5116 2044 1996 5518 8372 1012 1999 2294 1010 2009 2001 4991 2000 20910 27585 2015 1012 1996 5318 1998 2962 8508 3961 15704 1012 1996 7720 1998 3221 4443 2003 2036 2019 19401 2051 3538 1012 2172 1997 1996 7720 2001 10964 2013 6954 1010 3304 1012 7720 10714 2015 2024 2145 1999 2224 2090 8158 1012 2045 2003 1037 9882 2691 4412 11134 2007 9677 10902 1998 21006 4264 1010 10747 1998 10223 22373 2752 1012 2023 7891 2723 3131 2003 1999 1996 4373 1997 1996 2311 1010 1998 2038 6581 9325 1012 1996 2434 9746 2806 3016 3645 2024 2019 3181 2112 1997 2023 3131 1012 1037 3959 4292 1012 2624 3799 2542 2012 2009 1005 1055 2190 1012 2023 3131 2003 3155 21267 2675 2519 1998 2003 1037 3819 7822 2005 2147 2030 3942 2000 2624 3799 1012 1996 7570 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.291712 4584494528 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.294615 4584494528 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.296591 4584494528 tf_logging.py:115] label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.303467 4584494528 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.304759 4584494528 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] classic sf victorian half mile from glen park bart subway station , 15 min drive to sf ##o , 4 subway stops to powell st cable car / downtown sf . free parking available upon request . sf city short term registration number : st ##r - ( phone number hidden ) you will have access to the 2 joined bedrooms , a living room , dining room , kitchen , and newly remodeled bathroom . there will also be a wash ##er , dry ##er and wi ##fi . we don ' t have cable , but you are welcome to connect your laptop to the tv . the garage available to park in front . we live in a non - meter ##ed area , but every wednesday from 10 - 12 ##pm we have street sweeping . we provide all the towels , linen ##s and usual toilet ##ries ( toilet paper , iron , etc ) found at hotels . feel free to message me if you have any questions . normally , i ’ m fairly quick to respond : ) the neighborhood is residential , but provides easy access to sf ##o or downtown sf . there is an asian grocery store right next store that stocks anything you may need ( and then some ! ) . right around glen park [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.306581 4584494528 tf_logging.py:115] tokens: [CLS] classic sf victorian half mile from glen park bart subway station , 15 min drive to sf ##o , 4 subway stops to powell st cable car / downtown sf . free parking available upon request . sf city short term registration number : st ##r - ( phone number hidden ) you will have access to the 2 joined bedrooms , a living room , dining room , kitchen , and newly remodeled bathroom . there will also be a wash ##er , dry ##er and wi ##fi . we don ' t have cable , but you are welcome to connect your laptop to the tv . the garage available to park in front . we live in a non - meter ##ed area , but every wednesday from 10 - 12 ##pm we have street sweeping . we provide all the towels , linen ##s and usual toilet ##ries ( toilet paper , iron , etc ) found at hotels . feel free to message me if you have any questions . normally , i ’ m fairly quick to respond : ) the neighborhood is residential , but provides easy access to sf ##o or downtown sf . there is an asian grocery store right next store that stocks anything you may need ( and then some ! ) . right around glen park [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 4438 16420 6652 2431 3542 2013 8904 2380 12075 10798 2276 1010 2321 8117 3298 2000 16420 2080 1010 1018 10798 6762 2000 8997 2358 5830 2482 1013 5116 16420 1012 2489 5581 2800 2588 5227 1012 16420 2103 2460 2744 8819 2193 1024 2358 2099 1011 1006 3042 2193 5023 1007 2017 2097 2031 3229 2000 1996 1016 2587 18390 1010 1037 2542 2282 1010 7759 2282 1010 3829 1010 1998 4397 27170 5723 1012 2045 2097 2036 2022 1037 9378 2121 1010 4318 2121 1998 15536 8873 1012 2057 2123 1005 1056 2031 5830 1010 2021 2017 2024 6160 2000 7532 2115 12191 2000 1996 2694 1012 1996 7381 2800 2000 2380 1999 2392 1012 2057 2444 1999 1037 2512 1011 8316 2098 2181 1010 2021 2296 9317 2013 2184 1011 2260 9737 2057 2031 2395 12720 1012 2057 3073 2035 1996 24213 1010 17517 2015 1998 5156 11848 5134 1006 11848 3259 1010 3707 1010 4385 1007 2179 2012 9275 1012 2514 2489 2000 4471 2033 2065 2017 2031 2151 3980 1012 5373 1010 1045 1521 1049 7199 4248 2000 6869 1024 1007 1996 5101 2003 5647 1010 2021 3640 3733 3229 2000 16420 2080 2030 5116 16420 1012 2045 2003 2019 4004 13025 3573 2157 2279 3573 2008 15768 2505 2017 2089 2342 1006 1998 2059 2070 999 1007 1012 2157 2105 8904 2380 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.308145 4584494528 tf_logging.py:115] input_ids: 101 4438 16420 6652 2431 3542 2013 8904 2380 12075 10798 2276 1010 2321 8117 3298 2000 16420 2080 1010 1018 10798 6762 2000 8997 2358 5830 2482 1013 5116 16420 1012 2489 5581 2800 2588 5227 1012 16420 2103 2460 2744 8819 2193 1024 2358 2099 1011 1006 3042 2193 5023 1007 2017 2097 2031 3229 2000 1996 1016 2587 18390 1010 1037 2542 2282 1010 7759 2282 1010 3829 1010 1998 4397 27170 5723 1012 2045 2097 2036 2022 1037 9378 2121 1010 4318 2121 1998 15536 8873 1012 2057 2123 1005 1056 2031 5830 1010 2021 2017 2024 6160 2000 7532 2115 12191 2000 1996 2694 1012 1996 7381 2800 2000 2380 1999 2392 1012 2057 2444 1999 1037 2512 1011 8316 2098 2181 1010 2021 2296 9317 2013 2184 1011 2260 9737 2057 2031 2395 12720 1012 2057 3073 2035 1996 24213 1010 17517 2015 1998 5156 11848 5134 1006 11848 3259 1010 3707 1010 4385 1007 2179 2012 9275 1012 2514 2489 2000 4471 2033 2065 2017 2031 2151 3980 1012 5373 1010 1045 1521 1049 7199 4248 2000 6869 1024 1007 1996 5101 2003 5647 1010 2021 3640 3733 3229 2000 16420 2080 2030 5116 16420 1012 2045 2003 2019 4004 13025 3573 2157 2279 3573 2008 15768 2505 2017 2089 2342 1006 1998 2059 2070 999 1007 1012 2157 2105 8904 2380 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.309695 4584494528 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.313158 4584494528 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.315030 4584494528 tf_logging.py:115] label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.321314 4584494528 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.322968 4584494528 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] specials ask about 2 nt ##s or more specials private room w / private bathroom in a renovated boutique hotel not a private home . free wi - fi . free deluxe continental breakfast . parking paid by the guest directly to the hotel . this is a times ##har ##e property , inn at the opera you are not ob ##liga ##ted to attend a times ##har ##e presentation at any time the hotel unit ( ballet ) is the studio unit and averages 200 square feet . sleeps two people , the hotel unit has one queen size bed and a private bathroom . the rooms are equipped with cable television , a nightstand with a lamp , a desk or round table with two chairs and a kitchen ##ette ( no stove ) . central air conditioning is not offered , but the rooms have ceiling fans , and extra portable fans can be requested at the front desk . the kitchen ##ette has a microwave oven , ( no stove ) coffee ##maker , refrigerator flat ##ware , glass ##ware , and silver ##ware . queen size bed ( s ) with pillow top mattress ##es arm ##oire closet kitchen ##ette with refrigerator , microwave and coffee ##maker ( n [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.324537 4584494528 tf_logging.py:115] tokens: [CLS] specials ask about 2 nt ##s or more specials private room w / private bathroom in a renovated boutique hotel not a private home . free wi - fi . free deluxe continental breakfast . parking paid by the guest directly to the hotel . this is a times ##har ##e property , inn at the opera you are not ob ##liga ##ted to attend a times ##har ##e presentation at any time the hotel unit ( ballet ) is the studio unit and averages 200 square feet . sleeps two people , the hotel unit has one queen size bed and a private bathroom . the rooms are equipped with cable television , a nightstand with a lamp , a desk or round table with two chairs and a kitchen ##ette ( no stove ) . central air conditioning is not offered , but the rooms have ceiling fans , and extra portable fans can be requested at the front desk . the kitchen ##ette has a microwave oven , ( no stove ) coffee ##maker , refrigerator flat ##ware , glass ##ware , and silver ##ware . queen size bed ( s ) with pillow top mattress ##es arm ##oire closet kitchen ##ette with refrigerator , microwave and coffee ##maker ( n [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 19247 3198 2055 1016 23961 2015 2030 2062 19247 2797 2282 1059 1013 2797 5723 1999 1037 10601 24611 3309 2025 1037 2797 2188 1012 2489 15536 1011 10882 1012 2489 15203 6803 6350 1012 5581 3825 2011 1996 4113 3495 2000 1996 3309 1012 2023 2003 1037 2335 8167 2063 3200 1010 7601 2012 1996 3850 2017 2024 2025 27885 14715 3064 2000 5463 1037 2335 8167 2063 8312 2012 2151 2051 1996 3309 3131 1006 7250 1007 2003 1996 2996 3131 1998 20185 3263 2675 2519 1012 25126 2048 2111 1010 1996 3309 3131 2038 2028 3035 2946 2793 1998 1037 2797 5723 1012 1996 4734 2024 6055 2007 5830 2547 1010 1037 23135 2007 1037 10437 1010 1037 4624 2030 2461 2795 2007 2048 8397 1998 1037 3829 7585 1006 2053 16247 1007 1012 2430 2250 14372 2003 2025 3253 1010 2021 1996 4734 2031 5894 4599 1010 1998 4469 12109 4599 2064 2022 7303 2012 1996 2392 4624 1012 1996 3829 7585 2038 1037 18302 17428 1010 1006 2053 16247 1007 4157 8571 1010 18097 4257 8059 1010 3221 8059 1010 1998 3165 8059 1012 3035 2946 2793 1006 1055 1007 2007 10005 2327 13342 2229 2849 26250 9346 3829 7585 2007 18097 1010 18302 1998 4157 8571 1006 1050 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.326288 4584494528 tf_logging.py:115] input_ids: 101 19247 3198 2055 1016 23961 2015 2030 2062 19247 2797 2282 1059 1013 2797 5723 1999 1037 10601 24611 3309 2025 1037 2797 2188 1012 2489 15536 1011 10882 1012 2489 15203 6803 6350 1012 5581 3825 2011 1996 4113 3495 2000 1996 3309 1012 2023 2003 1037 2335 8167 2063 3200 1010 7601 2012 1996 3850 2017 2024 2025 27885 14715 3064 2000 5463 1037 2335 8167 2063 8312 2012 2151 2051 1996 3309 3131 1006 7250 1007 2003 1996 2996 3131 1998 20185 3263 2675 2519 1012 25126 2048 2111 1010 1996 3309 3131 2038 2028 3035 2946 2793 1998 1037 2797 5723 1012 1996 4734 2024 6055 2007 5830 2547 1010 1037 23135 2007 1037 10437 1010 1037 4624 2030 2461 2795 2007 2048 8397 1998 1037 3829 7585 1006 2053 16247 1007 1012 2430 2250 14372 2003 2025 3253 1010 2021 1996 4734 2031 5894 4599 1010 1998 4469 12109 4599 2064 2022 7303 2012 1996 2392 4624 1012 1996 3829 7585 2038 1037 18302 17428 1010 1006 2053 16247 1007 4157 8571 1010 18097 4257 8059 1010 3221 8059 1010 1998 3165 8059 1012 3035 2946 2793 1006 1055 1007 2007 10005 2327 13342 2229 2849 26250 9346 3829 7585 2007 18097 1010 18302 1998 4157 8571 1006 1050 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.328104 4584494528 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.330111 4584494528 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:19:59.331791 4584494528 tf_logging.py:115] label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 1788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.710801 4584494528 tf_logging.py:115] Writing example 0 of 1788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.717175 4584494528 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.718499 4584494528 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] fill ##more heritage address : ( phone number hidden by air ##bn ##b ) fill ##more street building : fill ##more heritage neighborhood : western addition bedrooms : 1 br bathrooms : 1 ba style : condo parking : garage square feet : 81 ##0 lease term : flexible 1 - 12 month lease terms comfortable and quiet condo near pacific heights this bright and spacious one bedroom unit boasts 81 ##0 square feet , 9 ft high ceilings , and beautiful hardwood floors throughout . the highlights of the living area are the views japan ##town . the go ##ur ##met kitchen includes granite counter tops , stainless steel appliances , and studio becker cabinets . there is a hallway coat closet adjacent to the wash ##er / dry ##er closet . one car parking space are included with the unit . pg & e paid up to $ 65 after which tenant pays . unit features : • youtube tv • high ceilings • 81 ##0 sq . ft . per owner • kitchen includes stainless steel appliances , microwave and dish ##wash ##er • lots of closet space • spacious master bedroom with plenty of natural sunlight • hardwood fl ##o [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.720218 4584494528 tf_logging.py:115] tokens: [CLS] fill ##more heritage address : ( phone number hidden by air ##bn ##b ) fill ##more street building : fill ##more heritage neighborhood : western addition bedrooms : 1 br bathrooms : 1 ba style : condo parking : garage square feet : 81 ##0 lease term : flexible 1 - 12 month lease terms comfortable and quiet condo near pacific heights this bright and spacious one bedroom unit boasts 81 ##0 square feet , 9 ft high ceilings , and beautiful hardwood floors throughout . the highlights of the living area are the views japan ##town . the go ##ur ##met kitchen includes granite counter tops , stainless steel appliances , and studio becker cabinets . there is a hallway coat closet adjacent to the wash ##er / dry ##er closet . one car parking space are included with the unit . pg & e paid up to $ 65 after which tenant pays . unit features : • youtube tv • high ceilings • 81 ##0 sq . ft . per owner • kitchen includes stainless steel appliances , microwave and dish ##wash ##er • lots of closet space • spacious master bedroom with plenty of natural sunlight • hardwood fl ##o [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 6039 5974 4348 4769 1024 1006 3042 2193 5023 2011 2250 24700 2497 1007 6039 5974 2395 2311 1024 6039 5974 4348 5101 1024 2530 2804 18390 1024 1015 7987 28942 1024 1015 8670 2806 1024 25805 5581 1024 7381 2675 2519 1024 6282 2692 10084 2744 1024 12379 1015 1011 2260 3204 10084 3408 6625 1998 4251 25805 2379 3534 7535 2023 4408 1998 22445 2028 5010 3131 21979 6282 2692 2675 2519 1010 1023 3027 2152 21098 1010 1998 3376 23165 8158 2802 1012 1996 11637 1997 1996 2542 2181 2024 1996 5328 2900 4665 1012 1996 2175 3126 11368 3829 2950 9753 4675 13284 1010 18676 3886 22449 1010 1998 2996 15309 20053 1012 2045 2003 1037 6797 5435 9346 5516 2000 1996 9378 2121 1013 4318 2121 9346 1012 2028 2482 5581 2686 2024 2443 2007 1996 3131 1012 18720 1004 1041 3825 2039 2000 1002 3515 2044 2029 16713 12778 1012 3131 2838 1024 1528 7858 2694 1528 2152 21098 1528 6282 2692 5490 1012 3027 1012 2566 3954 1528 3829 2950 18676 3886 22449 1010 18302 1998 9841 28556 2121 1528 7167 1997 9346 2686 1528 22445 3040 5010 2007 7564 1997 3019 9325 1528 23165 13109 2080 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.721985 4584494528 tf_logging.py:115] input_ids: 101 6039 5974 4348 4769 1024 1006 3042 2193 5023 2011 2250 24700 2497 1007 6039 5974 2395 2311 1024 6039 5974 4348 5101 1024 2530 2804 18390 1024 1015 7987 28942 1024 1015 8670 2806 1024 25805 5581 1024 7381 2675 2519 1024 6282 2692 10084 2744 1024 12379 1015 1011 2260 3204 10084 3408 6625 1998 4251 25805 2379 3534 7535 2023 4408 1998 22445 2028 5010 3131 21979 6282 2692 2675 2519 1010 1023 3027 2152 21098 1010 1998 3376 23165 8158 2802 1012 1996 11637 1997 1996 2542 2181 2024 1996 5328 2900 4665 1012 1996 2175 3126 11368 3829 2950 9753 4675 13284 1010 18676 3886 22449 1010 1998 2996 15309 20053 1012 2045 2003 1037 6797 5435 9346 5516 2000 1996 9378 2121 1013 4318 2121 9346 1012 2028 2482 5581 2686 2024 2443 2007 1996 3131 1012 18720 1004 1041 3825 2039 2000 1002 3515 2044 2029 16713 12778 1012 3131 2838 1024 1528 7858 2694 1528 2152 21098 1528 6282 2692 5490 1012 3027 1012 2566 3954 1528 3829 2950 18676 3886 22449 1010 18302 1998 9841 28556 2121 1528 7167 1997 9346 2686 1528 22445 3040 5010 2007 7564 1997 3019 9325 1528 23165 13109 2080 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.723896 4584494528 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.725338 4584494528 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.726654 4584494528 tf_logging.py:115] label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.734596 4584494528 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.747461 4584494528 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this brand new 1 bedroom + bathroom is in the gorgeous re ##mo ##dell ##ed home ~ west portal with free guest parking , steps from l - tara ##val mu ##ni metro , restaurants , shops and bars and walking distance to stern grove park . huge bedroom w / queen size bed , w / 800 count sheets , and fluffy white towels . office work space - 5 ##g wi ##fi . 2 single beds , flat screen tv - hu ##lu + netflix . kitchen - fully equipped & stocked . ~ l - tara ##val metro is one block from the house will get you downtown market st ##r & sf zoo / ocean beach . this brand new remodeled edward ##ian house in the prime location of san francisco near west portal - restaurants and boutique ##s mecca , stern grove and sf zoo + ocean beach . the home offers on - site free parking and plenty of free street parking . also , we are just one block away from metro l - tara ##val mu ##ni station . you will enjoy our home as it offers private bedroom that is ideal for families or friends up to 4 people . the place is very clean and have 1 . 5 bathrooms ( with the guest bathroom on the main level [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.751818 4584494528 tf_logging.py:115] tokens: [CLS] this brand new 1 bedroom + bathroom is in the gorgeous re ##mo ##dell ##ed home ~ west portal with free guest parking , steps from l - tara ##val mu ##ni metro , restaurants , shops and bars and walking distance to stern grove park . huge bedroom w / queen size bed , w / 800 count sheets , and fluffy white towels . office work space - 5 ##g wi ##fi . 2 single beds , flat screen tv - hu ##lu + netflix . kitchen - fully equipped & stocked . ~ l - tara ##val metro is one block from the house will get you downtown market st ##r & sf zoo / ocean beach . this brand new remodeled edward ##ian house in the prime location of san francisco near west portal - restaurants and boutique ##s mecca , stern grove and sf zoo + ocean beach . the home offers on - site free parking and plenty of free street parking . also , we are just one block away from metro l - tara ##val mu ##ni station . you will enjoy our home as it offers private bedroom that is ideal for families or friends up to 4 people . the place is very clean and have 1 . 5 bathrooms ( with the guest bathroom on the main level [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 4435 2047 1015 5010 1009 5723 2003 1999 1996 9882 2128 5302 12662 2098 2188 1066 2225 9445 2007 2489 4113 5581 1010 4084 2013 1048 1011 10225 10175 14163 3490 6005 1010 7884 1010 7340 1998 6963 1998 3788 3292 2000 8665 7676 2380 1012 4121 5010 1059 1013 3035 2946 2793 1010 1059 1013 5385 4175 8697 1010 1998 27036 2317 24213 1012 2436 2147 2686 1011 1019 2290 15536 8873 1012 1016 2309 9705 1010 4257 3898 2694 1011 15876 7630 1009 20907 1012 3829 1011 3929 6055 1004 24802 1012 1066 1048 1011 10225 10175 6005 2003 2028 3796 2013 1996 2160 2097 2131 2017 5116 3006 2358 2099 1004 16420 9201 1013 4153 3509 1012 2023 4435 2047 27170 3487 2937 2160 1999 1996 3539 3295 1997 2624 3799 2379 2225 9445 1011 7884 1998 24611 2015 21340 1010 8665 7676 1998 16420 9201 1009 4153 3509 1012 1996 2188 4107 2006 1011 2609 2489 5581 1998 7564 1997 2489 2395 5581 1012 2036 1010 2057 2024 2074 2028 3796 2185 2013 6005 1048 1011 10225 10175 14163 3490 2276 1012 2017 2097 5959 2256 2188 2004 2009 4107 2797 5010 2008 2003 7812 2005 2945 2030 2814 2039 2000 1018 2111 1012 1996 2173 2003 2200 4550 1998 2031 1015 1012 1019 28942 1006 2007 1996 4113 5723 2006 1996 2364 2504 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.756569 4584494528 tf_logging.py:115] input_ids: 101 2023 4435 2047 1015 5010 1009 5723 2003 1999 1996 9882 2128 5302 12662 2098 2188 1066 2225 9445 2007 2489 4113 5581 1010 4084 2013 1048 1011 10225 10175 14163 3490 6005 1010 7884 1010 7340 1998 6963 1998 3788 3292 2000 8665 7676 2380 1012 4121 5010 1059 1013 3035 2946 2793 1010 1059 1013 5385 4175 8697 1010 1998 27036 2317 24213 1012 2436 2147 2686 1011 1019 2290 15536 8873 1012 1016 2309 9705 1010 4257 3898 2694 1011 15876 7630 1009 20907 1012 3829 1011 3929 6055 1004 24802 1012 1066 1048 1011 10225 10175 6005 2003 2028 3796 2013 1996 2160 2097 2131 2017 5116 3006 2358 2099 1004 16420 9201 1013 4153 3509 1012 2023 4435 2047 27170 3487 2937 2160 1999 1996 3539 3295 1997 2624 3799 2379 2225 9445 1011 7884 1998 24611 2015 21340 1010 8665 7676 1998 16420 9201 1009 4153 3509 1012 1996 2188 4107 2006 1011 2609 2489 5581 1998 7564 1997 2489 2395 5581 1012 2036 1010 2057 2024 2074 2028 3796 2185 2013 6005 1048 1011 10225 10175 14163 3490 2276 1012 2017 2097 5959 2256 2188 2004 2009 4107 2797 5010 2008 2003 7812 2005 2945 2030 2814 2039 2000 1018 2111 1012 1996 2173 2003 2200 4550 1998 2031 1015 1012 1019 28942 1006 2007 1996 4113 5723 2006 1996 2364 2504 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.759538 4584494528 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.761568 4584494528 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.763514 4584494528 tf_logging.py:115] label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.769543 4584494528 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.772390 4584494528 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] come stay in this san francisco victorian , ( phone number hidden by air ##bn ##b ) frame and ( phone number hidden by air ##bn ##b ) re ##mo ##del . original mold ##ing details , and a sweet little back patio that looks out over the bay . this listing is for 2 queen / double sized beds . if you need more than 2 beds , use my 3 bed listing below . https : / / www . air ##bn ##b . com / rooms / 207 ##45 ##19 ##5 ? location = pot ##rer ##o % 20 ##hill % 2 ##c % 20s ##an % 20 ##fra ##nc ##is ##co % 2 ##c % 20 ##ca % 2 ##c % 20 ##uni ##ted % 20s ##tate ##s & check _ in = 2018 - 03 - 25 & check _ out = 2018 - 03 - 31 & s = au ##r ##q ##nk ##ve [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.774208 4584494528 tf_logging.py:115] tokens: [CLS] come stay in this san francisco victorian , ( phone number hidden by air ##bn ##b ) frame and ( phone number hidden by air ##bn ##b ) re ##mo ##del . original mold ##ing details , and a sweet little back patio that looks out over the bay . this listing is for 2 queen / double sized beds . if you need more than 2 beds , use my 3 bed listing below . https : / / www . air ##bn ##b . com / rooms / 207 ##45 ##19 ##5 ? location = pot ##rer ##o % 20 ##hill % 2 ##c % 20s ##an % 20 ##fra ##nc ##is ##co % 2 ##c % 20 ##ca % 2 ##c % 20 ##uni ##ted % 20s ##tate ##s & check _ in = 2018 - 03 - 25 & check _ out = 2018 - 03 - 31 & s = au ##r ##q ##nk ##ve [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2272 2994 1999 2023 2624 3799 6652 1010 1006 3042 2193 5023 2011 2250 24700 2497 1007 4853 1998 1006 3042 2193 5023 2011 2250 24700 2497 1007 2128 5302 9247 1012 2434 18282 2075 4751 1010 1998 1037 4086 2210 2067 19404 2008 3504 2041 2058 1996 3016 1012 2023 10328 2003 2005 1016 3035 1013 3313 7451 9705 1012 2065 2017 2342 2062 2084 1016 9705 1010 2224 2026 1017 2793 10328 2917 1012 16770 1024 1013 1013 7479 1012 2250 24700 2497 1012 4012 1013 4734 1013 19843 19961 16147 2629 1029 3295 1027 8962 14544 2080 1003 2322 7100 1003 1016 2278 1003 27074 2319 1003 2322 27843 12273 2483 3597 1003 1016 2278 1003 2322 3540 1003 1016 2278 1003 2322 19496 3064 1003 27074 12259 2015 1004 4638 1035 1999 1027 2760 1011 6021 1011 2423 1004 4638 1035 2041 1027 2760 1011 6021 1011 2861 1004 1055 1027 8740 2099 4160 8950 3726 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.775708 4584494528 tf_logging.py:115] input_ids: 101 2272 2994 1999 2023 2624 3799 6652 1010 1006 3042 2193 5023 2011 2250 24700 2497 1007 4853 1998 1006 3042 2193 5023 2011 2250 24700 2497 1007 2128 5302 9247 1012 2434 18282 2075 4751 1010 1998 1037 4086 2210 2067 19404 2008 3504 2041 2058 1996 3016 1012 2023 10328 2003 2005 1016 3035 1013 3313 7451 9705 1012 2065 2017 2342 2062 2084 1016 9705 1010 2224 2026 1017 2793 10328 2917 1012 16770 1024 1013 1013 7479 1012 2250 24700 2497 1012 4012 1013 4734 1013 19843 19961 16147 2629 1029 3295 1027 8962 14544 2080 1003 2322 7100 1003 1016 2278 1003 27074 2319 1003 2322 27843 12273 2483 3597 1003 1016 2278 1003 2322 3540 1003 1016 2278 1003 2322 19496 3064 1003 27074 12259 2015 1004 4638 1035 1999 1027 2760 1011 6021 1011 2423 1004 4638 1035 2041 1027 2760 1011 6021 1011 2861 1004 1055 1027 8740 2099 4160 8950 3726 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.777296 4584494528 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.778908 4584494528 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.780225 4584494528 tf_logging.py:115] label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.785526 4584494528 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.787224 4584494528 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] * great for professionals ! * private room with a queen size bed available in the quiet laurel heights neighborhood . shared bathroom , kitchen , and dining room w / desk . our location is convenient ##ly located near the following bus lines : - 38 ( su ##tro baths , financial district ) - 43 ( hai ##ght , fort mason and northern piers ) & - 1 ( chinatown and no ##b hill ) close to golden gate & pre ##si ##dio parks , and a block away from trader joe ' s and target ! we look forward to hosting you ! from 10 ##am - 3 ##pm on weekdays , guests will not have access to the house . however , guests staying multiple nights are welcome to leave their belongings in the home . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.788859 4584494528 tf_logging.py:115] tokens: [CLS] * great for professionals ! * private room with a queen size bed available in the quiet laurel heights neighborhood . shared bathroom , kitchen , and dining room w / desk . our location is convenient ##ly located near the following bus lines : - 38 ( su ##tro baths , financial district ) - 43 ( hai ##ght , fort mason and northern piers ) & - 1 ( chinatown and no ##b hill ) close to golden gate & pre ##si ##dio parks , and a block away from trader joe ' s and target ! we look forward to hosting you ! from 10 ##am - 3 ##pm on weekdays , guests will not have access to the house . however , guests staying multiple nights are welcome to leave their belongings in the home . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1008 2307 2005 8390 999 1008 2797 2282 2007 1037 3035 2946 2793 2800 1999 1996 4251 11893 7535 5101 1012 4207 5723 1010 3829 1010 1998 7759 2282 1059 1013 4624 1012 2256 3295 2003 14057 2135 2284 2379 1996 2206 3902 3210 1024 1011 4229 1006 10514 13181 19692 1010 3361 2212 1007 1011 4724 1006 15030 13900 1010 3481 6701 1998 2642 16067 1007 1004 1011 1015 1006 22321 1998 2053 2497 2940 1007 2485 2000 3585 4796 1004 3653 5332 20617 6328 1010 1998 1037 3796 2185 2013 17667 3533 1005 1055 1998 4539 999 2057 2298 2830 2000 9936 2017 999 2013 2184 3286 1011 1017 9737 2006 19759 1010 6368 2097 2025 2031 3229 2000 1996 2160 1012 2174 1010 6368 6595 3674 6385 2024 6160 2000 2681 2037 20033 1999 1996 2188 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.790564 4584494528 tf_logging.py:115] input_ids: 101 1008 2307 2005 8390 999 1008 2797 2282 2007 1037 3035 2946 2793 2800 1999 1996 4251 11893 7535 5101 1012 4207 5723 1010 3829 1010 1998 7759 2282 1059 1013 4624 1012 2256 3295 2003 14057 2135 2284 2379 1996 2206 3902 3210 1024 1011 4229 1006 10514 13181 19692 1010 3361 2212 1007 1011 4724 1006 15030 13900 1010 3481 6701 1998 2642 16067 1007 1004 1011 1015 1006 22321 1998 2053 2497 2940 1007 2485 2000 3585 4796 1004 3653 5332 20617 6328 1010 1998 1037 3796 2185 2013 17667 3533 1005 1055 1998 4539 999 2057 2298 2830 2000 9936 2017 999 2013 2184 3286 1011 1017 9737 2006 19759 1010 6368 2097 2025 2031 3229 2000 1996 2160 1012 2174 1010 6368 6595 3674 6385 2024 6160 2000 2681 2037 20033 1999 1996 2188 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.792253 4584494528 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.793947 4584494528 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.795485 4584494528 tf_logging.py:115] label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.802311 4584494528 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.803839 4584494528 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] located on liberty hill in the heart of dolores heights with close proximity to dolores park , valencia corridor , restaurants , shops , night ##life , and public transportation . great for young professionals , corporate rental ##s , families , and vacation ##ers . modern penthouse in sunny dolores heights on one of the most sought out streets in the city ! completely rebuilt in 2006 with great attention to detail , high - end furnishings , go ##ur ##met kitchen , and an open floor plan offering the best views ( 270 degree ) in the city . convenient location walking distance to dolores park , valencia corridor , the mission , no ##e valley and the castro . master bedroom has a king size te ##mp ##ra ##ped ##ic bed , plenty of closet space , a fireplace , yard access , and a 40 \" tv . bedroom 2 has a queen size heavenly bed , closet , full bath , and dresser . bedroom 3 ( and / or den ) has a contemporary sofa bed ( full ) , sectional l - shape couch , walk - in closet , bath with large shower , office no ##ok , yard access , and 61 \" tv . great for movie viewing [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.815078 4584494528 tf_logging.py:115] tokens: [CLS] located on liberty hill in the heart of dolores heights with close proximity to dolores park , valencia corridor , restaurants , shops , night ##life , and public transportation . great for young professionals , corporate rental ##s , families , and vacation ##ers . modern penthouse in sunny dolores heights on one of the most sought out streets in the city ! completely rebuilt in 2006 with great attention to detail , high - end furnishings , go ##ur ##met kitchen , and an open floor plan offering the best views ( 270 degree ) in the city . convenient location walking distance to dolores park , valencia corridor , the mission , no ##e valley and the castro . master bedroom has a king size te ##mp ##ra ##ped ##ic bed , plenty of closet space , a fireplace , yard access , and a 40 \" tv . bedroom 2 has a queen size heavenly bed , closet , full bath , and dresser . bedroom 3 ( and / or den ) has a contemporary sofa bed ( full ) , sectional l - shape couch , walk - in closet , bath with large shower , office no ##ok , yard access , and 61 \" tv . great for movie viewing [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2284 2006 7044 2940 1999 1996 2540 1997 21544 7535 2007 2485 10039 2000 21544 2380 1010 13083 7120 1010 7884 1010 7340 1010 2305 15509 1010 1998 2270 5193 1012 2307 2005 2402 8390 1010 5971 12635 2015 1010 2945 1010 1998 10885 2545 1012 2715 22855 1999 11559 21544 7535 2006 2028 1997 1996 2087 4912 2041 4534 1999 1996 2103 999 3294 7183 1999 2294 2007 2307 3086 2000 6987 1010 2152 1011 2203 23127 1010 2175 3126 11368 3829 1010 1998 2019 2330 2723 2933 5378 1996 2190 5328 1006 13756 3014 1007 1999 1996 2103 1012 14057 3295 3788 3292 2000 21544 2380 1010 13083 7120 1010 1996 3260 1010 2053 2063 3028 1998 1996 11794 1012 3040 5010 2038 1037 2332 2946 8915 8737 2527 5669 2594 2793 1010 7564 1997 9346 2686 1010 1037 13788 1010 4220 3229 1010 1998 1037 2871 1000 2694 1012 5010 1016 2038 1037 3035 2946 16581 2793 1010 9346 1010 2440 7198 1010 1998 16641 1012 5010 1017 1006 1998 1013 2030 7939 1007 2038 1037 3824 10682 2793 1006 2440 1007 1010 27197 1048 1011 4338 6411 1010 3328 1011 1999 9346 1010 7198 2007 2312 6457 1010 2436 2053 6559 1010 4220 3229 1010 1998 6079 1000 2694 1012 2307 2005 3185 10523 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.817492 4584494528 tf_logging.py:115] input_ids: 101 2284 2006 7044 2940 1999 1996 2540 1997 21544 7535 2007 2485 10039 2000 21544 2380 1010 13083 7120 1010 7884 1010 7340 1010 2305 15509 1010 1998 2270 5193 1012 2307 2005 2402 8390 1010 5971 12635 2015 1010 2945 1010 1998 10885 2545 1012 2715 22855 1999 11559 21544 7535 2006 2028 1997 1996 2087 4912 2041 4534 1999 1996 2103 999 3294 7183 1999 2294 2007 2307 3086 2000 6987 1010 2152 1011 2203 23127 1010 2175 3126 11368 3829 1010 1998 2019 2330 2723 2933 5378 1996 2190 5328 1006 13756 3014 1007 1999 1996 2103 1012 14057 3295 3788 3292 2000 21544 2380 1010 13083 7120 1010 1996 3260 1010 2053 2063 3028 1998 1996 11794 1012 3040 5010 2038 1037 2332 2946 8915 8737 2527 5669 2594 2793 1010 7564 1997 9346 2686 1010 1037 13788 1010 4220 3229 1010 1998 1037 2871 1000 2694 1012 5010 1016 2038 1037 3035 2946 16581 2793 1010 9346 1010 2440 7198 1010 1998 16641 1012 5010 1017 1006 1998 1013 2030 7939 1007 2038 1037 3824 10682 2793 1006 2440 1007 1010 27197 1048 1011 4338 6411 1010 3328 1011 1999 9346 1010 7198 2007 2312 6457 1010 2436 2053 6559 1010 4220 3229 1010 1998 6079 1000 2694 1012 2307 2005 3185 10523 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.819514 4584494528 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.821882 4584494528 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:20:15.825112 4584494528 tf_logging.py:115] label: 4 (id = 4)\n"
     ]
    }
   ],
   "source": [
    "# set max sequence length\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "# convert features to InputFeatures that BERT understands\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, \n",
    "                                                                  label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, \n",
    "                                                                  label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function for the model\n",
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels, num_labels):\n",
    "    \n",
    "    bert_module = hub.Module(\n",
    "        BERT_MODEL_HUB,\n",
    "        trainable=True)\n",
    "    bert_inputs = dict(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(\n",
    "        inputs=bert_inputs,\n",
    "        signature='tokens',\n",
    "        as_dict=True)\n",
    "    \n",
    "    # use pooled_output for classification tasks\n",
    "    output_layer = bert_outputs['pooled_output']\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "    \n",
    "    output_weights = tf.get_variable(\n",
    "        'output_weights', [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    \n",
    "    output_bias = tf.get_variable(\n",
    "        'output_bias', [num_labels], initializer=tf.zeros_initializer())\n",
    "    \n",
    "    with tf.variable_scope('loss'):\n",
    "        \n",
    "        # add dropout layer to prevent overfitting\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        \n",
    "        # one-hot encode labels\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "        \n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        # obtain predicted labels and probabilities\n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs)\n",
    "        \n",
    "        # compute loss between actual and predicted labels during evaluation\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, predicted_labels, log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap model function in model_fn_builder that allows model to train, evaluate and predict\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps, num_warmup_steps):\n",
    "    \n",
    "    def model_fn(features, labels, mode, params):\n",
    "        \n",
    "        input_ids = features['input_ids']\n",
    "        input_mask = features['input_mask']\n",
    "        segment_ids = features['segment_ids']\n",
    "        label_ids = features['label_ids']\n",
    "        \n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "        \n",
    "        if not is_predicting:\n",
    "            (loss, predicted_labels, log_probs) = create_model(\n",
    "                is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "            \n",
    "            train_op = bert.optimization.create_optimizer(\n",
    "                loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "            \n",
    "            # calculate evaluation metrics\n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                }\n",
    "            \n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "            \n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                    loss=loss,\n",
    "                    train_op=train_op)\n",
    "            else:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                    loss=loss,\n",
    "                    eval_metric_ops=eval_metrics)\n",
    "                \n",
    "        else:\n",
    "            (predicted_labels, log_probs) = create_model(\n",
    "                is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "            predictions = {\n",
    "                'probabilities': log_probs,\n",
    "                'labels': predicted_labels\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "WARMUP_PROPORTION = 0.1\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute number of training and warmup steps\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify output directory and checkpoints to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir='modelOutput',\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'learn-co/capstone', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a31c5bba8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:21:15.399399 4584494528 tf_logging.py:115] Using config: {'_model_dir': 'learn-co/capstone', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a31c5bba8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# build estimator\n",
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input function for training\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:22:33.721390 4584494528 tf_logging.py:115] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:22:37.773957 4584494528 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:22:46.951075 4584494528 tf_logging.py:115] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:22:46.955893 4584494528 tf_logging.py:115] Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:22:51.985491 4584494528 tf_logging.py:115] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:22:59.873231 4584494528 tf_logging.py:115] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:23:00.184877 4584494528 tf_logging.py:115] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into learn-co/capstone/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:23:15.318429 4584494528 tf_logging.py:115] Saving checkpoints for 0 into learn-co/capstone/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.3869125, step = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 21:25:13.790229 4584494528 tf_logging.py:115] loss = 1.3869125, step = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0101033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 00:10:11.476330 4584494528 tf_logging.py:115] global_step/sec: 0.0101033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.46236134, step = 101 (9897.779 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 00:10:11.557042 4584494528 tf_logging.py:115] loss = 0.46236134, step = 101 (9897.779 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0103301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 02:51:31.897533 4584494528 tf_logging.py:115] global_step/sec: 0.0103301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.3892298, step = 201 (9680.453 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 02:51:32.010720 4584494528 tf_logging.py:115] loss = 0.3892298, step = 201 (9680.453 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.010298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 05:33:22.559194 4584494528 tf_logging.py:115] global_step/sec: 0.010298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.48835307, step = 301 (9710.743 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 05:33:22.879728 4584494528 tf_logging.py:115] loss = 0.48835307, step = 301 (9710.743 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.00982725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 08:22:58.340095 4584494528 tf_logging.py:115] global_step/sec: 0.00982725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.10299247, step = 401 (10175.675 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 08:22:58.429893 4584494528 tf_logging.py:115] loss = 0.10299247, step = 401 (10175.675 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 500 into learn-co/capstone/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 11:18:40.378169 4584494528 tf_logging.py:115] Saving checkpoints for 500 into learn-co/capstone/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.00939172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 11:20:26.014056 4584494528 tf_logging.py:115] global_step/sec: 0.00939172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.1605391, step = 501 (10647.615 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 11:20:26.045171 4584494528 tf_logging.py:115] loss = 0.1605391, step = 501 (10647.615 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 502 into learn-co/capstone/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 11:22:18.215119 4584494528 tf_logging.py:115] Saving checkpoints for 502 into learn-co/capstone/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.28876072.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 11:22:28.397181 4584494528 tf_logging.py:115] Loss for final step: 0.28876072.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took time  14:00:00.211425\n"
     ]
    }
   ],
   "source": [
    "# begin training\n",
    "print(f'Beginning training')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 11:26:43.519441 4584494528 tf_logging.py:115] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 11:26:46.432037 4584494528 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 11:26:58.897428 4584494528 tf_logging.py:115] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-03-26-18:26:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 11:26:58.924485 4584494528 tf_logging.py:115] Starting evaluation at 2019-03-26-18:26:58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 11:27:00.387809 4584494528 tf_logging.py:115] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from learn-co/capstone/model.ckpt-502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 11:27:00.420941 4584494528 tf_logging.py:115] Restoring parameters from learn-co/capstone/model.ckpt-502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 11:27:04.216907 4584494528 tf_logging.py:115] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 11:27:04.423233 4584494528 tf_logging.py:115] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2019-03-26-18:54:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 11:54:22.889250 4584494528 tf_logging.py:115] Finished evaluation at 2019-03-26-18:54:22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 502: eval_accuracy = 0.81263983, global_step = 502, loss = 0.57338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 11:54:22.891098 4584494528 tf_logging.py:115] Saving dict for global step 502: eval_accuracy = 0.81263983, global_step = 502, loss = 0.57338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 502: learn-co/capstone/model.ckpt-502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 11:54:25.874749 4584494528 tf_logging.py:115] Saving 'checkpoint_path' summary for global step 502: learn-co/capstone/model.ckpt-502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_accuracy': 0.81263983, 'loss': 0.57338, 'global_step': 502}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create input function for evaluation\n",
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n",
    "\n",
    "# get evaluation metrics\n",
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to take new predictions\n",
    "def getPrediction(text):\n",
    "    labels = ['no_review', '0.0-1.0', '1.0-2.0', '2.0-3.0', '3.0-4.0', '4.0-5.0']\n",
    "    input_examples = [run_classifier.InputExample(guid=None, text_a = x, text_b = None, label = 0) for x in text]\n",
    "    input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "    predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "    predictions = estimator.predict(predict_input_fn)\n",
    "    return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(text, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert text from listing description\n",
    "text = ['''We live in an 1895 Victorian top flat with 12 foot ceilings on the best block in San Francisco. \n",
    "           Our calendar is always up to date. Location Location Location~ Our home is close to the Mission, \n",
    "           Lower Haight, and 2.5 blocks from the Castro theater. The underground is 3 blocks away, with a \n",
    "           street car even closer. The room is small and cosy, but a great price for one of the most \n",
    "           expensive neighborhoods in the US!''',\n",
    "    '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:28.473710 4584494528 tf_logging.py:115] Writing example 0 of 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:28.482618 4584494528 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:28.484205 4584494528 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] we live in an 1895 victorian top flat with 12 foot ceilings on the best block in san francisco . our calendar is always up to date . location location location ~ our home is close to the mission , lower hai ##ght , and 2 . 5 blocks from the castro theater . the underground is 3 blocks away , with a street car even closer . the room is small and co ##sy , but a great price for one of the most expensive neighborhoods in the us ! [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:28.487796 4584494528 tf_logging.py:115] tokens: [CLS] we live in an 1895 victorian top flat with 12 foot ceilings on the best block in san francisco . our calendar is always up to date . location location location ~ our home is close to the mission , lower hai ##ght , and 2 . 5 blocks from the castro theater . the underground is 3 blocks away , with a street car even closer . the room is small and co ##sy , but a great price for one of the most expensive neighborhoods in the us ! [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2057 2444 1999 2019 6301 6652 2327 4257 2007 2260 3329 21098 2006 1996 2190 3796 1999 2624 3799 1012 2256 8094 2003 2467 2039 2000 3058 1012 3295 3295 3295 1066 2256 2188 2003 2485 2000 1996 3260 1010 2896 15030 13900 1010 1998 1016 1012 1019 5991 2013 1996 11794 4258 1012 1996 5230 2003 1017 5991 2185 1010 2007 1037 2395 2482 2130 3553 1012 1996 2282 2003 2235 1998 2522 6508 1010 2021 1037 2307 3976 2005 2028 1997 1996 2087 6450 11681 1999 1996 2149 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:28.489713 4584494528 tf_logging.py:115] input_ids: 101 2057 2444 1999 2019 6301 6652 2327 4257 2007 2260 3329 21098 2006 1996 2190 3796 1999 2624 3799 1012 2256 8094 2003 2467 2039 2000 3058 1012 3295 3295 3295 1066 2256 2188 2003 2485 2000 1996 3260 1010 2896 15030 13900 1010 1998 1016 1012 1019 5991 2013 1996 11794 4258 1012 1996 5230 2003 1017 5991 2185 1010 2007 1037 2395 2482 2130 3553 1012 1996 2282 2003 2235 1998 2522 6508 1010 2021 1037 2307 3976 2005 2028 1997 1996 2087 6450 11681 1999 1996 2149 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:28.491415 4584494528 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:28.493573 4584494528 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:28.496069 4584494528 tf_logging.py:115] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:28.498632 4584494528 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:28.500611 4584494528 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:28.502501 4584494528 tf_logging.py:115] tokens: [CLS] [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:28.504387 4584494528 tf_logging.py:115] input_ids: 101 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:28.506588 4584494528 tf_logging.py:115] input_mask: 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:28.508644 4584494528 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:28.510653 4584494528 tf_logging.py:115] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:28.589406 4584494528 tf_logging.py:115] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:33.967031 4584494528 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:34.155665 4584494528 tf_logging.py:115] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:34.686092 4584494528 tf_logging.py:115] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from learn-co/capstone/model.ckpt-502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:34.690333 4584494528 tf_logging.py:115] Restoring parameters from learn-co/capstone/model.ckpt-502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:37.429188 4584494528 tf_logging.py:115] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0326 13:14:37.549880 4584494528 tf_logging.py:115] Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "# save results to predictions\n",
    "predictions = getPrediction(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('We live in an 1895 Victorian top flat with 12 foot ceilings on the best block in San Francisco. \\n           Our calendar is always up to date. Location Location Location~ Our home is close to the Mission, \\n           Lower Haight, and 2.5 blocks from the Castro theater. The underground is 3 blocks away, with a \\n           street car even closer. The room is small and cosy, but a great price for one of the most \\n           expensive neighborhoods in the US!',\n",
       " array([-6.959856 , -6.916402 , -6.4755964, -4.4082885, -0.0322696,\n",
       "        -4.1291037], dtype=float32),\n",
       " '3.0-4.0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Geospatial analysis\n",
    "\n",
    "**Supply**: Figure 1 highlights a geographically inbalanced supply of homes which is a combined result of:\n",
    "\n",
    "   1. **Tourist attractions**: There is a greater demand for homes on the eastern side of the city because it is closer to tourist attractions, downtown and overall commercial activity. As we move over to West San Francisco, it becomes more residential with fewer attractions. Homes in West and South San Francisco are set at a lower price as a result. An exception to this general trend is the Tenderloin, a high-crime neighborhood, which acts to drive down price.\n",
    "    \n",
    "   2. **Lifestyle disparities**: It is also possible that there is a lower supply of homes in West San Francisco due to lifestyle disparities. In West San Francisco, homes are primarily occupied by families in contrast to the young professionals in East San Francisco. The latter group is more open to renting out any free space they have, while families will want to maintain their privacy. As a result, there are fewer available spaces in West San Francisco.\n",
    "    \n",
    "   3. **Age gap**: Related to lifestyle disparities is age gap, which may play a role in the adoption of Airbnb. As mentioned previously, West San Francisco is a mostly residential neighborhood with an older population. These individuals may be more reluctant to rent out their homes to strangers, and slow to embrace the idea of home-sharing. \n",
    "    \n",
    "\n",
    "**Price**: The impact of the inbalanced supply of homes is reflected in the price. Homes are more expensive in Northeast San Francisco due to the combined influences of the above factors. \n",
    "\n",
    "\n",
    "**Rating**: The effect of rating on price is most clearly demonstrated in South San Francisco (Ocean View, Portola, Bayview), where lower ratings coincide with low prices. This effect is mitigated somewhat in the Tenderloin, due to better ratings in the surrounding favorable neighborhoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Term associations\n",
    "\n",
    "In Figure 2, one of the more interesting points is how geographical information is captured in the scatterplot. Neighborhoods like Bernal Heights/Castro are more commonly associated with high star ratings, while Cole Valley/Chinatown are associated with all other ratings. Notice how Bernal Heights/Castro have a higher median price than Cole Valley/Chinatown, further reinforcing the notion that price and star rating are related.\n",
    "\n",
    "\n",
    "The most differentiating words are private homes and properties owned by home-share companies (Sonder & Zeus). Private homes are more commonly associated with a high star rating, while Sonder & Zeus are more frequently used in all other ratings. This suggests people may have a better experience with private homes, because it feels more organic/natural. It is more in line with Airbnb's vision of co-inhabiting another individual's home, and immersing yourself in their culture, rather than a company-owned property. \n",
    "\n",
    "\n",
    "Listing descriptions with high star ratings also tend to feature the home with words such as: **victorian, backyard, garden, deck, private entrance** and **yard**. In contrast, listing descriptions with all other ratings focus more on the amenities of the property with words like: **bedding, wifi, linens, screen tv, email,** and **fitness**. While it is important to list amenities, these same words could also be used to describe a hotel, and does little to describe the individuality of a home. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Lexical categories\n",
    "\n",
    "In Figure 3, the most frequent topic in listing descriptions with high star ratings is **royalty**. This theme is repeated 136 times/25,000 terms versus only 77 times/25,000 terms for all other ratings. In other words, listing descriptions with high star ratings use words associated with the theme royalty almost twice as much as all other ratings! Some of these words include, magnificent, queen, elegant, supreme and distinguished. \n",
    "\n",
    "But this is slightly misleading when taken out of context. While some of the listing descriptions may use words that grammatically match the theme royalty, the majority of these words are **queen** and **king**. They are used instead to describe the size of the bed. Still, these terms are used nearly twice as much, and it can be argued that listing descriptions including more mentions of luxuriously-sized beds, are more likely to attract a higher rating among guests. \n",
    "    \n",
    "\n",
    "The most frequent topic in listing descriptions with all other star ratings is **stealing**. Curiously, **property** appears to be the seed term for this lexical category. Property is mentioned 108 times/25,000 terms versus 26 times/25,000 terms for high star ratings. Examining the excerpts show that property is most commonly used in listing descriptions by other home-sharing companies like Sonder & Zeus. These companies refer to their living spaces as properties, rather than homes. This is fascinating because it reveals distinct linguistic disparities. Private homeowners don't view their homes as property, but a **home**, with all of its associated homey connotations. On the other hand, companies view these same homes as **property**, another asset in their portfolio. \n",
    "\n",
    "Interestingly, we see less **property** mentions in listing descriptions with high star ratings, which goes in hand with the above analysis that private homes enjoy higher ratings than properties owned by companies.\n",
    "    \n",
    "    \n",
    "When looking at some of the other terms on the list such as negative_emotion, violence and nervousness, keep in mind these categories have been taken out of context. For example, violence was inferred from: \n",
    "    \n",
    "*the city's **beating** heart, **kick** back in the suite on its own private floor, **hit** up The Mill for some liquid energy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Word similarities\n",
    "\n",
    "The plot in Figure 4 reveals two semi-distinct shapes, a central cluster (all other ratings) and an outer ring structure (high star ratings).\n",
    "\n",
    "\n",
    "Geography is clustered in the upper right corner, with neighborhoods and outdoor attractions, like the pier and Alcatraz. Transportation is captured to the right of the plot, with words like Lyft, Muni, and Bart. It is interesting to see how similar words are clustered, and then whether these words belong to highly rated descriptions or all others. For example, one of my favorite word similarities is how closely **pet** (associated with all other ratings) and **kid** (associated with high star rating) are clustered together. Homes more accommodating towards kids, are more favorable, while pets are rarely allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 BERT text classifier\n",
    "\n",
    "The model achieved 81.3% accuracy on a validation set of 1,780 listing descriptions. \n",
    "\n",
    "When I tested the BERT classifier on text from a listing description, it claimed the listing would fall into the 3-4 star category. The concern here though is that each category step is quite wide. A home receiving 3 stars, and a home receiving 4 stars would both belong in the same category but obviously, there is a significant gap between these two. A 3-star home would be much less preferable than a 4-star home. Currently, the rating system does not allow me to capture these finer in-between scoring steps, and I recommend obtaining a score of at least 4-5 stars to be confident in your listing description. \n",
    "\n",
    "There is also an issue of score distributions. It is unlikely for low-rated homes to remain on the platform for long, and as a result, nearly all of the homes on Airbnb have 3 or more stars. This creates an even more difficult task for the classifier. Homes with a weak listing description may receive the same star rating as a home with a much stronger, and stylistic description. As long as the guest feels like their stay was fine, regardless of the listing description, the home will receive an average or above-average rating. As a result, the model must learn that sometimes even weak listing descriptions will yield a decent rating. \n",
    "\n",
    "This may lead you to wonder then, why should hosts even bother with a listing description? The answer is because the listing description plays an important role in the probability of guests booking a home. When all the ratings are fairly similar, we begin to compare other factors, like how stylistic and unique a listing description is. Stronger listing descriptions increases the probability of a guest booking a home. Weaker listing descriptions may only have a history of three or four guests staying there, even though they have an above-average rating. Therefore, it is still important to write a great listing description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Recommendations\n",
    "\n",
    "1. Focus on describing what makes your home unique, instead of amenities. Does it have a garden for guests to sit out in and enjoy? Or perhaps a deck with a gorgeous view of the city? These types of descriptions are more apt to earn a high star rating.\n",
    "\n",
    "\n",
    "2. Describe your home with terms that evoke feelings of warmth and home, rather than detailing a list of amenities, and describing your space in an impersonal style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Next steps\n",
    "\n",
    "A couple projects that could build off of this body of work include:\n",
    "\n",
    "1. Creating a text generator in the Airbnb Plus editorial style, to make it even easier for hosts to quickly create a top-rated listing description.\n",
    "\n",
    "\n",
    "2. Training a classifier on a more stratified rating system to help hosts diagnose their listing descriptions in greater detail.\n",
    "\n",
    "\n",
    "3. These listing descriptions are unique to San Francisco, and it would be interesting to compare it with listing descriptions from another city, to observe regional discrepancies in what makes a home attractive. This could also be a fascinating study in how other cultures perceive their homes, as reflected in their text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
